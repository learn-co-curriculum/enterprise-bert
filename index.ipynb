{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyw76f7xnUA6"
      },
      "source": [
        "# BERT (Paired-Programming Exercise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zxTqNgdncka"
      },
      "source": [
        "**Objectives**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIfBXs3Fnhf9"
      },
      "source": [
        "*   To understand how BERT works\n",
        "*   To apply BERT to NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-IDzrNtnyRz"
      },
      "source": [
        "## BERT - The model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some details on the model"
      ],
      "metadata": {
        "id": "aD8dY5qBvmFO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tUa4M_1n0s5"
      },
      "source": [
        "**BERT** (**Bidrectional Encoder Representation from Transformer**) is a linguistic embedding model published by Google. It is a context-based model, unlike other embedding models such as word2vec, which are context-free. The context-sensitive nature of BERT was built upon a dataset of 3.3 billion words, in particular approximately 2.5 billion from Wikipedia and the balance from Google's [BookCorpus](https://www.english-corpora.org/googlebooks/#)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvw4Ond2o7Lb"
      },
      "source": [
        "Based on our previous discussion of the transformer, we can see where the terms \"encoder representation from transformer\" come from. But what about \"Bidirectional?\" Bidrectional simply mean the encoder can read the sentence in both directions, e.g. both *Cogito ergo sum* to *I think therefore I am* and vice versa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlbXDgW7ppSx"
      },
      "source": [
        "BERT has three main hyperparameters\n",
        "\n",
        "*   $ L $ is the number of encoder layers\n",
        "*   $ A $ is the number of attention heads\n",
        "*   $ H $ is the number of hidden units\n",
        "\n",
        "The model also comes in some pre-specified configurations, and here are the two standard ones\n",
        "\n",
        "*   BERT-base: $ L = 12 $, $ A = 12 $, $ H = 768 $\n",
        "*   BERT-large: $ L = 42 $, $ A = 16 $, $ H = 1,024 $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivs-fjb6mJLK"
      },
      "source": [
        "In particular, we'll be using BERT to help discover the missing word in a sentence. BERT can also be used for translation and Next Sentence Prediction (NSP) as well as a myriad of other applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW6veCBhtBJV"
      },
      "source": [
        "### Using BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZqY5HCstHLA"
      },
      "source": [
        "With that as prologue, let's start using BERT. First, we'll have to set up our environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSUywSCmtQCi"
      },
      "source": [
        "#### BERT's environment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** We are using Google Colab since with standard Jupyter notebooks, there can be a lot of issues with the various installations working well together; this is especially true for M1 chip MacBooks."
      ],
      "metadata": {
        "id": "Dw460S_CwIU8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4RVgZxymJLN"
      },
      "outputs": [],
      "source": [
        "# Install transformers\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_-VRcyK6mJLO"
      },
      "outputs": [],
      "source": [
        "# Import the german libraries\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model ```bert-base-uncased``` is one of the pretrained BERT models and it has 110 million parameters. Details can be found at [Hugging Face](https://huggingface.co/bert-base-uncased).\n",
        "\n"
      ],
      "metadata": {
        "id": "ee95WDWUqx7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Masking with BERT"
      ],
      "metadata": {
        "id": "xOKkOJePt0zZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMZ0uxw9mJLO"
      },
      "outputs": [],
      "source": [
        "# Define our function unmasker\n",
        "unmasker = pipeline('fill-mask', model='bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try a sentence and see how BERT does."
      ],
      "metadata": {
        "id": "ifpztwnVrUv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [MASK] goes in the place you want BERT to predict the correct word\n",
        "unmasker(\"Artificial Intelligence [MASK] take over the world.\")"
      ],
      "metadata": {
        "id": "cmtc5eFvrbhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top five possibilities are shown. Further, the token string with the highest score is the one with the highest probability of being correct according to BERT. In this example, it is \"can\" as in \"artificial intelligence can take over the world.\"\n",
        "\n",
        "On supposes we should be happy that \"can\" has a higher probability than \"will.\""
      ],
      "metadata": {
        "id": "HmeYmafrr0YK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the output, ```token``` refers to the position of the masked token in the list that is generated from the transformer. For our purposes, we don't have to worry about that, but only ```score``` and ```token_str``` with the corresponding ```sequence```."
      ],
      "metadata": {
        "id": "gyhfWVTdx-Z4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Task 1: Masking Twice"
      ],
      "metadata": {
        "id": "V6FJiAOEuwUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happens if one used ```[MASK]``` two times in a sentence?\n",
        "\n",
        "For example, run the following in the code block below and interpret the results.\n",
        "\n",
        "\n",
        "```\n",
        "unmasker(\"Artificial Intelligence [MASK] take over the [MASK}.\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "YLlGwl47uz0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using [MASK] twice\n"
      ],
      "metadata": {
        "id": "RWhx076avLEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explain and interpret the \"double-mask\" here.*"
      ],
      "metadata": {
        "id": "cKL6Ii09vNt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Task 2: Using unmasker"
      ],
      "metadata": {
        "id": "wC_lcDyEtxNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use ```unmasker``` on three other sentences. At least one of them should be a \"double-mask.\"\n",
        "Explain and interpret each one."
      ],
      "metadata": {
        "id": "54H5Q0C7uAkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run each unmasker sentence in a different code cell followed by their analysis in a text cell.\n"
      ],
      "metadata": {
        "id": "zX5ehNazw3d5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Literary Interlude"
      ],
      "metadata": {
        "id": "0Bp1iLO70AtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does ```unmasker``` perform with a quote from literature or other notable work?\n",
        "\n"
      ],
      "metadata": {
        "id": "RQZ0NIj30lrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look first a \"To be, or not to be, that is the question\" from William Shakespeare's *Hamlet* (Act 3, Scene 1)."
      ],
      "metadata": {
        "id": "dS52iXYW1sj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's mask \"question\"\n",
        "unmasker(\"To be, or not to be, that is the [MASK]:\")"
      ],
      "metadata": {
        "id": "vLwSjMcz1doQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the highest probability does give us the correct answer.\n",
        "\n",
        "Let's look at another one."
      ],
      "metadata": {
        "id": "7US4PB6X2EH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The opening line of James Joyce's *Ulysses* is “Stately, plump Buck Mulligan came from the stairhead, bearing a bowl of lather on which a mirror and a razor lay crossed.”"
      ],
      "metadata": {
        "id": "V59GFvzW1ZsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's mask \"plump\"\n",
        "unmasker(\"Stately, [MASK] Buck Mulligan came from the stairhead, bearing a bowl of lather on which a mirror and a razor lay crossed.\")"
      ],
      "metadata": {
        "id": "PuJzHtB_0BdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the actual word- \"plump\"- did not make the top 5.\n",
        "\n",
        "Now let's unmask \"plump\" and mask \"lather.\""
      ],
      "metadata": {
        "id": "oYFKkbs6061i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let'colabs mask \"later\"\n",
        "unmasker(\"Stately, plump Buck Mulligan came from the stairhead, bearing a bowl of [MASK] on which a mirror and a razor lay crossed.\")"
      ],
      "metadata": {
        "id": "5WrDDMaO1FD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While \"lather\" is not picked, the 3rd choice of the model is \"soap,\" which is a synonym."
      ],
      "metadata": {
        "id": "MbLG6bos1N9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Task 3: A quote from literature or other notable work"
      ],
      "metadata": {
        "id": "natdCaNM21oK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is your turn.\n",
        "\n",
        "Find a quote from literature or other notable work such as from a philosophical or religious text and make sure to state where the quote is from.\n",
        "\n",
        "Mask at least two different words and see how BERT performs."
      ],
      "metadata": {
        "id": "ygLPuZeR235l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "l35dhYQF3SEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Task 4: Bias in the model"
      ],
      "metadata": {
        "id": "q93qMYBJw1fi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following two code cells."
      ],
      "metadata": {
        "id": "1gvoJQY-w_59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Men at work\n",
        "unmasker(\"The man worked as a [MASK].\")"
      ],
      "metadata": {
        "id": "K5Ypy-sNxPyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Women at work\n",
        "unmasker(\"The man worked as a [MASK].\")"
      ],
      "metadata": {
        "id": "jCkwMLXOxUL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you notice about the top five responses for men and women? Explain. \n",
        "\n",
        "*Recall that we noted above which data BERT was trained on, so you may want to reference that in your explanation.*"
      ],
      "metadata": {
        "id": "3QBupf1ayHnx"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}